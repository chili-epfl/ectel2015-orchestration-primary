---
title: "Studying Teacher Orchestration Load in Technology-Enhanced Classrooms: A Mixed-method Approach and Case Study"
author: "Luis P. Prieto, Kshitij Sharma, Yun Wen & Pierre Dillenbourg"
output: html_document
---

This is an R Markdown document to reproduce the main data preprocessing, analysis and visualization for the homonymous [ECTEL2015 conference](http://ectel2015.httc.de/) paper. Below, we reproduce the abstract of the paper, a summary of the context and methods of the studies, and then the analysis and visualization of results from the studies.

## Abstract

Teacher orchestration of technology-enhanced learning processes plays a major role in students’ outcomes, especially in face-to-face classrooms. However, very few studies look into the fine-grained details of how such orchestration unfolds, the challenges and cognitive overload that using technologies at a classroom level pose for teachers. This paper proposes a mixed-method approach to the study of orchestration cognitive load, combining physio-behavioural (eye-tracking) and subjective measurements (questionnaires, stimulated recall interviews). We illustrate the approach by applying it to study the orchestration of two technology-enhanced geometry lessons, by a secondary school teacher. The results of our mixed-method analyses highlight the difficulty of classroom-level (as opposed to individual- or group-level) interactions, and especially modelling students’ progress and understanding. Such insights can be useful in the design of new classroom technologies, and to focus researchers’ attention on critical orchestration episodes during their evaluation.

## Context and Method

The study comprised two secondary education geometry sessions that took place in an international school near Lausanne (Switzerland). The two sessions followed the same general structure, and were run by the same teacher (a practitioner with more than 15 years of teaching experience), with two different cohorts of students, of 22 and 23 students respectively (aged 11-12 years old). During the 80-minute sessions, the teacher guided the students in individual and group work about geometrical figures and tessellations, using laptops and specialized geometry software ([Geometer's Sketchpad](http://www.keycurriculum.com/)), interspersed with small periods of explanation/lecturing. To support her orchestration of the lesson, the teacher was using a projector connected to her computer and the school's usual software for classroom management ([NetSupport School](http://www.netsupportschool.com/)).

We employed multiple data gathering techniques, in order to understand the orchestration process at different granularity levels: 

* **session-level subjective ratings of cognitive load** (including both quantitative questionnaires such as [NASA TLX](http://humansystems.arc.nasa.gov/groups/tlx/) and open-ended questions) were performed just after the session
* **The teacher's gaze** was recorded in-situ using a mobile eye-tracker, for fine-grained physiological measures at the milliseconds-level, but also for later analysis by researchers (at the 10-second episode level)
* An (audio-recorded) **stimulated recall interview** was conducted with the teacher, using the subjective video feed of the lesson to rate a selection of episodes from the lesson, using a think-aloud protocol to explain her rationale for the subjective ratings

The data analysis process will generally be as follows:

1. **From samples to episodes (eyetracking)**. Four eye-tracking measurements which have been related with cognitive load (pupil diameter mean, pupil diameter standard deviation, number of long fixations and average saccade speed) are calculated throughout the session. Each of these four measurements is aggregated into 10-second episodes (thus bridging from sample- to episode-level), and the median value of these aggregated measurements for the session is calculated. By performing a median cut (i.e., for each 10-second episode, how many of the four measurements are over the session's median), a ``load index'' ranging from 0 to 4 is calculated, indicating the likelihood that a certain 10-second episode represented a higher load than the session average (see [Prieto et al. (2014)](http://dl.acm.org/citation.cfm?id=2669543) for a more detailed explanation of this process).
2. **Subjective episode analysis (stimulated recall)**. From this collection of 10-second episodes (and their associated load index based on eye-tracking measures), a subset (e.g., 10 episodes) is selected from consistently high- or low-load periods during the session (five each), and 10-second video snippets with the subjective camera view of the teacher are generated for each. These videos are then used in a post-hoc stimulated recall interview with the teacher, in which the teacher is asked to rate subjectively each snippet using a standard 9-level mental effort scale, using a think-aloud protocol to express the rationale behind each rating. The numerical ratings are used to triangulate the load index obtained using eye-tracking, and the think-aloud output goes through qualitative analysis for triangulation and interpretation at higher levels.
3. **Objective episode analysis (video coding)**. All the extreme load index episodes (ELEs, i.e., those 10-second episodes with values 0 or 4) are then video coded by the researcher team, along three main dimensions characterizing orchestration (see table below): the _activity_ being performed by the teacher (e.g., explanation, monitoring), the _social plane_ of the interaction (e.g., class-wide, with individual students) and the _main focus of the teacher's gaze_ (a student laptop, students' faces, etc.). The video code counts are then aggregated for the whole session (thus bridging from the episode- to the session-level), and statistical tests (Pearson's chi-squared) are used to determine which coding dimensions and which video codes contribute most significantly to the differences between high- and low-load episodes, and using these codes to create distinct profiles for each. 
4. **Session-level analysis**. Finally, the session-level subjective ratings provided by the teacher (both quantitative scales and qualitative open responses) are triangulated with the qualitative data coming from the stimulated recall interview, to understand the overall perception of (cognitive) orchestration load at the session-level. These ratings can also be used to make comparisons among different sessions (e.g., which of two sessions was more difficult, and why). These inter-session comparisons can also be triangulated by comparing the medians used for the load index cut in step 1 above (after normalization by the first-episode values to cancel out physiological or ambiental differences among different sessions). 


Orchestration dimension | Teacher activity | Social plane | Main gaze focus
------------------------|------------------|--------------|----------------
Example video codes | Explanation/Lecturing (EXP), Monitoring (MON), Task distribution or transition (TDT), Technical or conceptual repairs (REP)... | Individual (IND), Small group (GRP), Class-wide (CLS) | Students’ faces (FAC) or backs (BAK), Tabletop surface (TAB), Paper worksheet (PAP)...


## Before starting: Data download and pre-processing

First of all, we download the dataset for the study, which has been published in Zenodo ([Dataset for Study (ISL2014BASELINE) MISSING LINK]())

```{r}
# We load the useful scripts and packages needed throughout the report
source("./lib/rollingWindows.R")
source("./lib/loadIndex.R")
source("./lib/extremeLoadExtraction.R")

rootdir <- getwd()
# If not present already, download dataset and uncompress it
setwd(paste(rootdir,"/data",sep=""))
# TODO: complete once the datasets are uploaded
# if(!file.exists())...
# unzip/bz2...
```

We now have all the datafiles uncompressed. This includes mainly a time series of eyetracking data for each of the two sessions, plus an additional set of files with the fixation details and saccade details of each session. There is also another file with the video-codes assigned to a fraction of the of 10-second episodes (those with extreme load indices), as coded by a single human researcher, following the three-dimension video coding scheme mentioned above. The raw gaze video data itself has not been made available due to anonymity reasons. 

Once we uncompress the data, we run the pre-processing of data,  calculating the four eyetracking variables of interest (see previous section), over 10-second rolling windows with 5-second slide, and then merge that data with the video coding data (see ```./data/Study2/preprocessStudy2Data.R``` and ```./lib/rollingWindows.R``` files for details).

```{r, cache=FALSE, message=FALSE, warning=FALSE}
setwd(paste(rootdir,"/data",sep=""))
source("preprocessData.R")
# We do the preprocessing, which will generate a Rda file with the 10s
# window data, and will return the name of the file

cleandatafile <- "ISL2014BASELINE-ProcessedData.Rda"
if(!file.exists(cleandatafile)){
    sessions <- c("JDC2014-Session1-eyetracking","JDC2014-Session2-eyetracking","JDC2014-Session3-eyetracking")
    
    # TODO: delete this, the uncompression is done at the top
    uncompressneeded <- FALSE    
    for (session in sessions){
        #We check whether we have all the raw data elements, uncompressed
        filename1 <- paste(session,"-eventexport.txt",sep="")
        filename2 <- paste(session,"-fixationDetails.txt",sep="")
        filename3 <- paste(session,"-saccadeDetails.txt",sep="")
        if(!file.exists(filename1) || !file.exists(filename2) || !file.exists(filename3)) uncompressneeded <- TRUE
    }
    
    if(uncompressneeded) unzip("JDC2014-EyetrackingData.zip")                
    data <- preprocessStudy2()
}

```



## 1. From samples to episodes (eyetracking)






















As a first step in our exploration of eye-tracking to follow teacher cognitive load, and taking into account that eye responses are often task-dependent, we devised a first _test for the validity of the measurements proposed by Buettner in a totally different task_. Using existing eye-tracking data from a previous experiment in which participants played a computer game (unrelated to the task in Buettner’s study), we estimated the evolution of the CL of participants throughout their experience, to see whether the results were consistent with what we knew about the game task in question and its temporal evolution.

### Data pre-processing

We uncompress the data from the study (warning, it is almost 5G of files!). This includes mainly a time series of eyetracking data along with the value of several game variables (e.g., mean Tetris stack height) in each instant, plus an additional set of files with the fixation details of each game in the experiment. 

Once we uncompress the data, we run the pre-processing of data: basically, separating the data for each game, and calculating the four eyetracking variables of interest, over 10-second rolling windows with 5-second slide:

* Pupil diameter mean
* Pupil diameter standard deviation
* Number of fixations with duration >500ms
* Average saccade speed

(see ```./data/Study1/preprocessStudy1Data.R``` and ```./lib/rollingWindows.R``` files for details)

```{r, cache=TRUE, message=FALSE, warning=FALSE}
setwd(paste(rootdir,"/data/Study1",sep=""))
source("preprocessStudy1Data.R")
# We do the preprocessing, which will generate a Rda file with the 10s
# window data, and will return the name of the file
cleandatafile <- "Study1ProcessedData.Rda"

# TODO: delete this, the uncompression is done at the top
if(!file.exists(cleandatafile)){
    if(!file.exists("ALLCombinedVariables_timeseriesPupilEvolution.csv")){
        unzip("ALLCombinedVariables_timeseriesPupilEvolution.csv.zip")                
    }
    unzip("FixationDetails.zip")
    preprocessStudy1()
}
```

### Data analysis

The main basic data analysis we do here is just to calculate the Load Index for each 10s window in a game (see ```./lib/loadIndex.R``` file for details):

```{r}
# We load the overall dataset with the data from the 10s sliding windows
totaldata <- get(load(paste(rootdir,"/data/Study1/",cleandatafile,sep="")))

# This dataframe will contain the data with the added Load Index data
loaddata <- data.frame()

# We calculate the load index, considering each game a different session
# for the effects of the median cut
games = unique(totaldata$gameID[])
for(game in games){
    data <- totaldata[totaldata$gameID == game,]
    # We add the columns with Load Index data
    newdata <- calculateLoadIndexSession(data)    
    # We join the new data into a dataset with all session's data
    if(length(loaddata)==0) loaddata <- newdata
    else loaddata <- rbind(loaddata,newdata)
}       

# Now, loaddata will contain the data to be summarized/visualized

```


### Main Results and visualization

If we look at the mean values of Load index and different game variables (mainly, the Tetris mean stack height and its variance)

**1. Temporal evolution**

```{r, fig.height=4, fig.width=7, message=FALSE, warning=FALSE}
require("gplots")
require("ggplot2")

# We can plot the Load Index graph of one such game, to understand how it looks like
gamedata <- loaddata[loaddata$gameID == games[1],]
ggplot(gamedata, aes(x=time, y=Load, col=Load)) + 
            ggtitle(paste("Load Index, single game\n(estimation of cognitive overload over 10s)",sep="")) + 
            geom_line(size=1) +
            theme(axis.text.x = element_blank(),plot.title=element_text(size=20, face="bold"),axis.title=element_text(size=18),panel.background = element_rect(fill = 'white')) +
            scale_color_gradient(low="green",high="red")

```

We can see that for a single game the load index is quite noise, but has a certain descending trend over time. It may be more interesting to look at the aggregates of all the games we recorded, to see the average evolution of Load Index over time, and also the evolution of the average main game metrics over time:

```{r, fig.height=4, fig.width=7}

# TODO: What was the code for this graph - Figure 1a in the paper??

```

We can see the average cognitive load index (brown curve) of participants as time went on, as well as the temporal evolution of the stack height (green curve) and stack variance (blue curve). If we think in terms of the particular task (the Tetris game), we get interesting insights into how the cognitive load may evolve over time: at the beginning (low mean stack heights) the cognitive load is high (as many alternative options for placing a new piece are open to decide amongst), and it generally goes down as the game goes towards the end (higher mean stack height), until we eventually disengage from the game when we give up. Similar but opposite is the effect of stack variance (higher variance implies more complex stack profiles, difficult to process and with more open alternatives for placement of the next block).

**2. Load Index vs. Game variables (stack height mean/variance)**

```{r, message=FALSE, warning=FALSE}
plotmeans(loaddata$value.stackMean~loaddata$Load, 
          xlab="Load Index",
            ylab="Mean stack height (avg. 10s episode)",
            main="Load Index vs. Stack height", barwidth=2)
plotmeans(loaddata$value.stackVar~loaddata$Load,
          xlab="Load Index",
            ylab="Stack height variance (avg. 10s episode)",
            main="Load Index vs. Stack height", barwidth=2)
```

The two figures showing main descriptive statistics of both game variables on 10-second windows (classified by their load index of each episode), show that the Load Index is positively correlated with stack variance, and negatively correlated with the mean stack height, and that such an effect is more clearly apparent in the extreme values of the load index.

### Summary of results

These results show that the Load Index, computed as described above, has potential for distinguishing different kinds of episodes occurring during the task (represented by moments with different mean stack heights and variances). We also see how CL may be related with the amount of open alternatives in each moment (in a sense, the uncertainty or the ‘entropy’ we perceive about the current game situation).

## Study 2 (Semi-authentic study): multi-tabletops at an open-doors day in the lab

For the next study we aimed to explore the following research questions: _is it feasible to use a mobile eye-tracker to follow CL in a semi-authentic classroom setting? Does such analysis provide interesting insights about classroom usability of a novel CSCL technology? Can we detect specific classroom interaction episodes that imply high (or low) cognitive load?_.

### Data pre-processing

If needed, we uncompress the data from the study. This includes mainly a time series of eyetracking data for each of the three sessions, plus an additional set of files with the fixation details and saccade details of each session. There is also another file with the codes assigned to a fraction of the of 10-second episodes (those with extreme load indices), as coded by a single human researcher, following the three-dimension video coding scheme mentioned above. The raw gaze video data itself has not been made available due to anonymity reasons.

Once we uncompress the data, we run the pre-processing of data,  calculating the four eyetracking variables of interest (see previous section), over 10-second rolling windows with 5-second slide, and then merge that data with the video coding data (see ```./data/Study2/preprocessStudy2Data.R``` and ```./lib/rollingWindows.R``` files for details).

```{r, cache=FALSE, message=FALSE, warning=FALSE}
setwd(paste(rootdir,"/data/Study2",sep=""))
source("preprocessStudy2Data.R")
# We do the preprocessing, which will generate a Rda file with the 10s
# window data, and will return the name of the file

cleandatafile <- "Study2ProcessedData.Rda"
if(!file.exists(cleandatafile)){
    sessions <- c("JDC2014-Session1-eyetracking","JDC2014-Session2-eyetracking","JDC2014-Session3-eyetracking")
    
    # TODO: delete this, the uncompression is done at the top
    uncompressneeded <- FALSE    
    for (session in sessions){
        #We check whether we have all the raw data elements, uncompressed
        filename1 <- paste(session,"-eventexport.txt",sep="")
        filename2 <- paste(session,"-fixationDetails.txt",sep="")
        filename3 <- paste(session,"-saccadeDetails.txt",sep="")
        if(!file.exists(filename1) || !file.exists(filename2) || !file.exists(filename3)) uncompressneeded <- TRUE
    }
    
    if(uncompressneeded) unzip("JDC2014-EyetrackingData.zip")                
    data <- preprocessStudy2()
}

```


### Data analysis
The main basic data analysis we do here is just to calculate the Load Index for each 10s window in a game (see ```./lib/loadIndex.R``` file for details):

```{r, warning=FALSE}
# We load the overall dataset with the data from the 10s sliding windows
totaldata <- get(load(paste(rootdir,"/data/Study2/",cleandatafile,sep="")))
sessions <- c("JDC2014-Session1-eyetracking","JDC2014-Session2-eyetracking","JDC2014-Session3-eyetracking")

# This dataframe will contain the data with the added Load Index data
loaddata <- data.frame()

# We calculate the load index, considering each session separately for the median cut
for(session in sessions){
    newdata <- data.frame()
    data <- totaldata[totaldata$Session == session,]
    # We add the columns with Load Index data
    newdata <- calculateLoadIndexSession(data, meanlabel="value.Mean", sdlabel="value.SD", fixlabel="value.Fix", saclabel="value.Sac")    
    # We join the new data into a dataset with all session's data
    if(length(loaddata)==0) loaddata <- newdata
    else loaddata <- rbind(loaddata,newdata)
}       

# Now, loaddata will contain the data to be summarized/visualized
```

Once we have this load index, the extreme load moments (those with Load Index equal to 0 or 4) are extracted in order to be video coded by a researcher, using the video feed from the eyetracker. The code to generate the csv file to be filled in by the coder is in the ```./lib/extremeLoadExtraction.R``` file:

```{r, warning=FALSE}
outputfile <- paste(rootdir,"/data/Study2/TimesToVideoCode.csv",sep="")
extremedata <- extractExtremeLoadMoments(sessions, loaddata, outputfile)
```

Once the video files are coded, the output should be in a csv file such as the one provided in our dataset. We then match those video codes with our load index data coming from the eyetracking:

```{r, warning=FALSE}
videosessions <- c("JDC2014-Session1-videocoding","JDC2014-Session2-videocoding","JDC2014-Session3-videocoding")

videodata <- data.frame()
for(videosession in videosessions){
    data <- read.csv(paste(rootdir,"/data/Study2/",videosession,".csv",sep=""))
    if(nrow(videodata)==0) videodata <- data   
    else videodata <- rbind(videodata,data)
}

videocodeddata <- merge(loaddata,videodata,by = c("Session","time"), all = T)

# We ensure that the load and video coded variables are factors, for later analysis, and eliminate any levels of the factor not used,
# and keep only the extreme load cases (in the dataset there are codes for some non-extreme values too)
videocodeddata <- videocodeddata[complete.cases(videocodeddata) & (videocodeddata$Load == 4 | videocodeddata$Load == 0),]
videocodeddata$Load <- as.factor(videocodeddata$Load)
videocodeddata$Activity <- factor(videocodeddata$Activity)
videocodeddata$Social <- factor(videocodeddata$Social)
videocodeddata$Focus <- factor(videocodeddata$Focus)
```

### Main results

In this study, we mainly want to see if the high- and low-load index episodes are significatively different, in terms of the video codes assigned to them (the teacher's activity, the social plane of interaction for that activity, and the main gaze focus).

#### Teacher activity

```{r, warning=FALSE}
tabAct <- table(videocodeddata$Load,videocodeddata$Activity)
tabAct
```

We can see the activity profiles for high- and low-load episodes are different but... how different? We perform a chi-squared test, and look at the residuals to see which video codes contribute significantly to these differences (e.g., which residuals are greater than 1.96):

```{r, warning=FALSE}
chisq.test(tabAct)
chisq.test(tabAct)$residuals
```

Thus, we can see that the differences _are statistically significant_, and we can see some trends in the high- and low-load episodes (low-load tend to be more monitoring and repair episodes; high-load tend to be more explanations and task transitions), although only the contribution of the TDT (task transition and distribution) to high-load episodes is significant.

#### Social plane of interaction

```{r, warning=FALSE}
tabSoc <- table(videocodeddata$Load,videocodeddata$Social)
tabSoc
```

We can see the social plane profiles for high- and low-load episodes are totally opposed in this case. We perform a chi-squared test, and look at the residuals to see which video codes contribute significantly to these differences (e.g., which residuals are greater than 1.96):

```{r, warning=FALSE}
chisq.test(tabSoc)
chisq.test(tabSoc)$residuals
```

Thus, we can see that the differences _are statistically significant_, and we can see that all video codes contribute significantly to these differences, meaning that _high-load episodes tend to be more often in the whole-class plane_, while _low-load episodes lie almost exclusively in the small-group plane_.


#### Main focus of teacher gaze

```{r, warning=FALSE}
tabFoc <- table(videocodeddata$Load,videocodeddata$Focus)
tabFoc
```

We can see the main focus profiles for high- and low-load episodes are also quite different. We perform a chi-squared test, and look at the residuals to see which video codes contribute significantly to these differences (e.g., which residuals are greater than 1.96):

```{r, warning=FALSE}
chisq.test(tabFoc)
chisq.test(tabFoc)$residuals
```

Thus, we can see that the differences _are statistically significant_, and we can see that all video codes contribute significantly to these differences, meaning that _high-load episodes tend to be more often in the whole-class plane_, while _low-load episodes lie almost exclusively in the small-group plane_.


### Summary of results

A statistical analysis (Pearson’s chi-squared test of independence) of the video coding for the episodes with minimum and maximum load index revealed that high-load and low-load episodes had statistically significant differentiated profiles in all three coding dimensions: teacher activity, activity's social plane and the main focus of teacher's gaze. By looking at the contributions of the different video codes to this statistical difference (the chi-squared test residuals for each code), we could identify the distinct profiles of high- and low-load episodes (i.e., what where the video codes appearing typically in one case or the other): high-load episodes had a higher chance of being transition/task distribution activities, and to occur at the classroom-level, featuring the students’ faces (e.g., when explaining) or backs (e.g., when monitoring the progress of activities), or the teacher’s own desk, which was cluttered with multiple paper elements to be distributed to the student groups as they progressed along the lesson activities. In contrast, low-load episodes occurred almost exclusively at the group social plane, while the teacher was focusing exclusively on one student tabletop.

The results of this study illustrate that it is feasible to use eye-tracking in a semi-authentic CSCL situation (e.g., the calibration procedure needed at the beginning of the eye-tracker recording could easily fit at the beginning of the lesson), and that the load index calculated using such techniques can be used to distinguish high/low load episodes. Furthermore, the study provided certain insights into classroom usability aspects of the specific CSCL technology used in this classroom: the need for classroom-level monitoring support in multi-tabletop classrooms and the dangers of a cluttered augmented paper user interface.





## Study 3 (Authentic study): master-level university course

For the last study, we set out to explore the following research questions: _is it feasible to use mobile eye-tracker to follow cognitive load in an authentic classroom/lesson?_ But also, since eye movement patterns can vary greatly from person to person and cognitive overload is known to be related to teaching expertise, we aimed at exploring a second question: _do teachers with different teaching experience show different load episode patterns?_.

### Data pre-processing

If needed, we uncompress the data from the study. This includes mainly a time series of eyetracking data for each of the three sessions, plus an additional set of files with the fixation details and saccade details of each session. There is also another file with the codes assigned to a fraction of the of 10-second episodes (those with extreme load indices), as coded by a single human researcher, following the three-dimension video coding scheme mentioned above. The raw gaze video data itself has not been made available due to anonymity reasons.

Once we uncompress the data, we run the pre-processing of data,  calculating the four eyetracking variables of interest (see previous section), over 10-second rolling windows with 5-second slide, and then merge that data with the video coding data (see ```./data/Study3/preprocessStudy3Data.R``` and ```./lib/rollingWindows.R``` files for details).

```{r, cache=FALSE, message=FALSE, warning=FALSE}
setwd(paste(rootdir,"/data/Study3",sep=""))
source("preprocessStudy3Data.R")
# We do the preprocessing, which will generate a Rda file with the 10s
# window data, and will return the name of the file

cleandatafile <- "Study3ProcessedData.Rda"
if(!file.exists(cleandatafile)){
    sessions <-  c("DELANA-Session1-Expert-eyetracking","DELANA-Session2-Expert-eyetracking","DELANA-Session3-Novice-eyetracking")
    
    # TODO: delete this, the uncompression is done at the top
    uncompressneeded <- FALSE    
    for (session in sessions){
        #We check whether we have all the raw data elements, uncompressed
        filename1 <- paste(session,"-eventexport.txt",sep="")
        filename2 <- paste(session,"-fixationDetails.txt",sep="")
        filename3 <- paste(session,"-saccadeDetails.txt",sep="")
        if(!file.exists(filename1) || !file.exists(filename2) || !file.exists(filename3)) uncompressneeded <- TRUE
    }
    
    if(uncompressneeded) unzip("DELANA-EyetrackingData.zip")                
    data <- preprocessStudy3()
}

```


### Data analysis
The main basic data analysis we do here is just to calculate the Load Index for each 10s window in a game (see ```./lib/loadIndex.R``` file for details):

```{r, warning=FALSE}
# We load the overall dataset with the data from the 10s sliding windows
totaldata <- get(load(paste(rootdir,"/data/Study3/",cleandatafile,sep="")))
sessions <-  c("DELANA-Session1-Expert-eyetracking","DELANA-Session2-Expert-eyetracking","DELANA-Session3-Novice-eyetracking")

# This dataframe will contain the data with the added Load Index data
loaddata <- data.frame()

# We calculate the load index, considering each session separately for the median cut
for(session in sessions){
    newdata <- data.frame()
    data <- totaldata[totaldata$Session == session,]
    # We add the columns with Load Index data
    newdata <- calculateLoadIndexSession(data, meanlabel="value.Mean", sdlabel="value.SD", fixlabel="value.Fix", saclabel="value.Sac")    
    # We join the new data into a dataset with all session's data
    if(length(loaddata)==0) loaddata <- newdata
    else loaddata <- rbind(loaddata,newdata)
}       

# Now, loaddata will contain the data to be summarized/visualized
```

Once we have this load index, the extreme load moments (those with Load Index equal to 0 or 4) are extracted in order to be video coded by a researcher, using the video feed from the eyetracker. The code to generate the csv file to be filled in by the coder is in the ```./lib/extremeLoadExtraction.R``` file:

```{r, warning=FALSE}
outputfile <- paste(rootdir,"/data/Study3/TimesToVideoCode.csv",sep="")
extremedata <- extractExtremeLoadMoments(sessions, loaddata, outputfile)
```

Once the video files are coded, the output should be in a csv file such as the one provided in our dataset. We then match those video codes with our load index data coming from the eyetracking:

```{r, warning=FALSE}
videodata <- read.csv(paste(rootdir,"/data/Study3/DELANA-videocoding.csv",sep=""))

videocodeddata <- merge(loaddata,videodata,by = c("Session","time"), all = T)

# We ensure that the load and video coded variables are factors, for later analysis, and eliminate any levels of the factor not used,
# and keep only the extreme load cases (in the dataset there are codes for some non-extreme values too)
videocodeddata <- videocodeddata[complete.cases(videocodeddata) & (videocodeddata$Load == 4 | videocodeddata$Load == 0),]
videocodeddata$Load <- as.factor(videocodeddata$Load)
videocodeddata$Activity <- factor(videocodeddata$Activity)
videocodeddata$Social <- factor(videocodeddata$Social)
videocodeddata$Focus <- factor(videocodeddata$Focus)
```


### Main results

In this study, we again want to see if, overall, the high- and low-load index episodes are significatively different, in terms of the video codes assigned to them (the teacher's activity, the social plane of interaction for that activity, and the main gaze focus). However, we also want to see if there are differences in the trends for the novice and the expert teachers that were recorded.

#### Overall load profiles and trends

We perform a similar video code count and statistical tests, as in the previous study. First, we look at the **teacher activity**:

```{r, warning=FALSE}
# For the teacher activity
tabAct <- table(videocodeddata$Load,videocodeddata$Activity)
tabAct
chisq.test(tabAct)
chisq.test(tabAct)$residuals
```

We can see the activity profiles for high- and low-load episodes are somehow different, but the difference is _not statistically significant_.

If we look at the **social plane** of interaction:

```{r, warning=FALSE}
# We merge individual and group social planes, as they are largely equivalent in the context of these sessions
levels(videocodeddata$Social) = c("CLS","GRPIND","GRPIND")
tabSoc <- table(videocodeddata$Load,videocodeddata$Social)
tabSoc
chisq.test(tabSoc)
chisq.test(tabSoc)$residuals
```

Thus, we can see that the differences _are statistically significant_, and although none of the codes is significant by itself, we can see that there is again a tendency of high-load episodes to be whole-class, while low-load episodes have a mix of individual or small-group plane, but also a large quantity of whole-class episodes among them.

Finally, we can look at the **main gaze focus**:

```{r, warning=FALSE}
tabFoc <- table(videocodeddata$Load,videocodeddata$Focus)
tabFoc
chisq.test(tabFoc)
chisq.test(tabFoc)$residuals
```

Thus, we can see that the differences _are statistically significant_, and we can see that there are several trends contributing to this difference, the strongest of which is the unlikeliness of having a high-load episode focused on the teacher's computer (TCOMP). Less significant trends include the focus on the teacher's own table in low-load episodes, or the focus on students' faces or the class's whiteboard during high-load episodes.


#### Per-participant load profiles and trends

As we see, the trends in this study are not as clear as in the previous one. Taking into account that this study includes the data of a novice and an expert teachers, we can try to analyze if the different teachers have different load episode patterns, even if the classroom dynamic was similar.

##### Novice teacher

We filter for the session recorded by the novice teacher, and perform a similar video code count and statistical tests. First, we look at the **teacher activity**:

```{r, warning=FALSE}
partdata <- videocodeddata[grepl("Novice",videocodeddata$Session),]

# We eliminate the video codes that do not appear for this participant
partdata$Activity <- factor(partdata$Activity)
# For the teacher activity
tabAct <- table(partdata$Load,partdata$Activity)
tabAct
chisq.test(tabAct)
chisq.test(tabAct)$residuals
```

We can see the activity profiles for high- and low-load episodes _are statistically significantly different_. Several trends seem to be contrib uting to this significance, especially explanation activities to high-load episodes, and solving problems or questions (i.e., repairs, REP) to low-load episodes.

If we look at the **social plane** of interaction:

```{r, warning=FALSE}
# We merge individual and group social planes, as they are largely equivalent in the context of these sessions
levels(partdata$Social) = c("CLS","GRPIND","GRPIND")
tabSoc <- table(partdata$Load,partdata$Social)
tabSoc
chisq.test(tabSoc)
chisq.test(tabSoc)$residuals
```

Thus, we can see that the differences _are statistically significant_, and although none of the codes is significant by itself, we can see that there is again a tendency of high-load episodes to be whole-class, while low-load episodes have a mix of individual or small-group plane, but also an appreciable quantity of whole-class episodes among them.

Finally, we can look at the **main gaze focus**:

```{r, warning=FALSE}
# We eliminate the video codes that do not appear for this participant
partdata$Focus <- factor(partdata$Focus)
# For the main focus of gaze
tabFoc <- table(partdata$Load,partdata$Focus)
tabFoc
chisq.test(tabFoc)
chisq.test(tabFoc)$residuals
```

Thus, we can see that the differences _are statistically significant_ (although barely so), and we can see that there are several trends contributing to this difference, such as the focus on the teacher's computer (TCOMP) in low-load episodes, or the focus on the faces of students (FAC) or the classroom's projector (PROJ) in the high-load episodes.


##### Expert teacher

We filter for the session recorded by the expert teacher, and perform a similar video code count and statistical tests. First, we look at the **teacher activity**:

```{r, warning=FALSE}
partdata <- videocodeddata[grepl("Expert",videocodeddata$Session),]

# We eliminate the video codes that do not appear for this participant
partdata$Activity <- factor(partdata$Activity)
# For the teacher activity
tabAct <- table(partdata$Load,partdata$Activity)
tabAct
chisq.test(tabAct)
chisq.test(tabAct)$residuals
```

We can see the activity profiles for high- and low-load episodes _not statistically significantly different_ in this case, and trends in this case are much less clear than in the previous ones.

If we look at the **social plane** of interaction:

```{r, warning=FALSE}
# We merge individual and group social planes, as they are largely equivalent in the context of these sessions
levels(partdata$Social) = c("CLS","GRPIND","GRPIND")
tabSoc <- table(partdata$Load,partdata$Social)
tabSoc
chisq.test(tabSoc)
chisq.test(tabSoc)$residuals
```

We can see that the differences are again _not statistically significant_, although we can see similar trends (e.g., it is relatively unlikely to have high-load episodes in a group-individual plane of interaction), only much weaker.

Finally, we can look at the **main gaze focus**:

```{r, warning=FALSE}
# We eliminate the video codes that do not appear for this participant
partdata$Focus <- factor(partdata$Focus)
# For the main focus of gaze
tabFoc <- table(partdata$Load,partdata$Focus)
tabFoc
chisq.test(tabFoc)
chisq.test(tabFoc)$residuals
```

In this case we see differences in high- and low-load episode profiles _are statistically significant_. Several trends are contributing to this difference, such as the focus on the teacher's computer (TCOMP) or the classroom's projector (PROJ) in low-load episodes, or the focus on the faces or backs of students (FAC, BAK) or the classroom's whiteboard (WHIT) in the high-load episodes.


### Summary of results

The results of this third study confirm some of the findings of the previous studies in this series (the increased load of classroom-level activities, or in trying to read students’ faces, probably to assess their understanding). They also served to spot cognitive load pattern differences between an expert and a novice teacher (novice’s high CL episodes being more concentrated in distinct kinds of activities/focus than the expert’s -- probably because the expert teacher was never really overloaded, and thus the Load Index will be a much noisier source of data). Finally, this study further confirms that this method for following CL of a teacher/facilitator is usable in authentic situations (although it requires the presence of a researcher/assistant for calibrating the device).

## Conclusions

Overall, the three aforementioned studies show that the proposed combination of mobile eye-tracking measurements can be feasibly used in authentic classroom settings. Together with post-hoc qualitative video coding, the approach also showed potential for discriminating fine-grained critical episodes (either high- or low-load) during the CSCL enactment, without having to rely on the teacher's memory of the events. The distinct profiles of such episodes have helped us gain insights into the difficulties of orchestrating CSCL activities: the need for awareness/monitoring support in multi-tabletop classrooms, and the importance of classroom-level awareness in general, the challenge that clutter and a scattered user interface pose in augmented paper applications. The results also show that the facilitation CL is highly dependent on the teacher's prior experience.

Our results across the three studies also provide a new insight about orchestration load, which should be explored in further studies: the fact that load seems to be correlated to the amount of "open alternatives" (i.e., the perceived uncertainty/entropy of the classroom situation). More novice teachers seemed to be especially sensitive to these high-uncertainty situations (e.g., looking at students faces during an explanation, trying to assess their comprehension; looking at students' backs while working in the tabletops, trying to assess their progress). This need of novice teachers for classroom management support can be used as a starting point for new technologies that can ameliorate the challenge of this kind of episodes (e.g., a system that helps novice lecturers to assess the attention or comprehension of their students).
