---
title: "Studying Teacher Orchestration Load in Technology-Enhanced Classrooms: A Mixed-method Approach and Case Study"
author: "Luis P. Prieto, Kshitij Sharma, Yun Wen & Pierre Dillenbourg"
output: html_document
---

This is an R Markdown document to reproduce the main data preprocessing, analysis and visualization for the homonymous [ECTEL2015 conference](http://ectel2015.httc.de/) paper. Below, we reproduce the abstract of the paper, a summary of the context and methods of the studies, and then the analysis and visualization of results from the studies.

## Abstract

Teacher orchestration of technology-enhanced learning processes plays a major role in students’ outcomes, especially in face-to-face classrooms. However, very few studies look into the fine-grained details of how such orchestration unfolds, the challenges and cognitive overload that using technologies at a classroom level pose for teachers. This paper proposes a mixed-method approach to the study of orchestration cognitive load, combining physio-behavioural (eye-tracking) and subjective measurements (questionnaires, stimulated recall interviews). We illustrate the approach by applying it to study the orchestration of two technology-enhanced geometry lessons, by a secondary school teacher. The results of our mixed-method analyses highlight the difficulty of classroom-level (as opposed to individual- or group-level) interactions, and especially modelling students’ progress and understanding. Such insights can be useful in the design of new classroom technologies, and to focus researchers’ attention on critical orchestration episodes during their evaluation.

## Context and Method

The study comprised two secondary education geometry sessions that took place in an international school near Lausanne (Switzerland). The two sessions followed the same general structure, and were run by the same teacher (a practitioner with more than 15 years of teaching experience), with two different cohorts of students, of 22 and 23 students respectively (aged 11-12 years old). During the 80-minute sessions, the teacher guided the students in individual and group work about geometrical figures and tessellations, using laptops and specialized geometry software ([Geometer's Sketchpad](http://www.keycurriculum.com/)), interspersed with small periods of explanation/lecturing. To support her orchestration of the lesson, the teacher was using a projector connected to her computer and the school's usual software for classroom management ([NetSupport School](http://www.netsupportschool.com/)).

We employed multiple data gathering techniques, in order to understand the orchestration process at different granularity levels: 

* **session-level subjective ratings of cognitive load** (including both quantitative questionnaires such as [NASA TLX](http://humansystems.arc.nasa.gov/groups/tlx/) and open-ended questions) were performed just after the session
* **The teacher's gaze** was recorded in-situ using a mobile eye-tracker, for fine-grained physiological measures at the milliseconds-level, but also for later analysis by researchers (at the 10-second episode level)
* An (audio-recorded) **stimulated recall interview** was conducted with the teacher, using the subjective video feed of the lesson to rate a selection of episodes from the lesson, using a think-aloud protocol to explain her rationale for the subjective ratings

The data analysis process will generally be as follows:

1. **From samples to episodes (eyetracking)**. Four eye-tracking measurements which have been related with cognitive load (pupil diameter mean, pupil diameter standard deviation, number of long fixations and average saccade speed) are calculated throughout the session. Each of these four measurements is aggregated into 10-second episodes (thus bridging from sample- to episode-level), and the median value of these aggregated measurements for the session is calculated. By performing a median cut (i.e., for each 10-second episode, how many of the four measurements are over the session's median), a ``load index'' ranging from 0 to 4 is calculated, indicating the likelihood that a certain 10-second episode represented a higher load than the session average (see [Prieto et al. (2014)](http://dl.acm.org/citation.cfm?id=2669543) for a more detailed explanation of this process).
2. **Subjective episode analysis (stimulated recall)**. From this collection of 10-second episodes (and their associated load index based on eye-tracking measures), a subset (e.g., 10 episodes) is selected from consistently high- or low-load periods during the session (five each), and 10-second video snippets with the subjective camera view of the teacher are generated for each. These videos are then used in a post-hoc stimulated recall interview with the teacher, in which the teacher is asked to rate subjectively each snippet using a standard 9-level mental effort scale, using a think-aloud protocol to express the rationale behind each rating. The numerical ratings are used to triangulate the load index obtained using eye-tracking, and the think-aloud output goes through qualitative analysis for triangulation and interpretation at higher levels.
3. **Objective episode analysis (video coding)**. All the extreme load index episodes (ELEs, i.e., those 10-second episodes with values 0 or 4) are then video coded by the researcher team, along three main dimensions characterizing orchestration (see table below): the _activity_ being performed by the teacher (e.g., explanation, monitoring), the _social plane_ of the interaction (e.g., class-wide, with individual students) and the _main focus of the teacher's gaze_ (a student laptop, students' faces, etc.). The video code counts are then aggregated for the whole session (thus bridging from the episode- to the session-level), and statistical tests (Pearson's chi-squared) are used to determine which coding dimensions and which video codes contribute most significantly to the differences between high- and low-load episodes, and using these codes to create distinct profiles for each. 
4. **Session-level analysis**. Finally, the session-level subjective ratings provided by the teacher (both quantitative scales and qualitative open responses) are triangulated with the qualitative data coming from the stimulated recall interview, to understand the overall perception of (cognitive) orchestration load at the session-level. These ratings can also be used to make comparisons among different sessions (e.g., which of two sessions was more difficult, and why). These inter-session comparisons can also be triangulated by comparing the medians used for the load index cut in step 1 above (after normalization by the first-episode values to cancel out physiological or ambiental differences among different sessions). 


Orchestration dimension | Teacher activity | Social plane | Main gaze focus
------------------------|------------------|--------------|----------------
Example video codes | Explanation/Lecturing (EXP), Monitoring (MON), Task distribution or transition (TDT), Technical or conceptual repairs (REP)... | Individual (IND), Small group (GRP), Class-wide (CLS) | Students’ faces (FAC) or backs (BAK), Tabletop surface (TAB), Paper worksheet (PAP)...


## Before starting: Data download

First of all, we download the dataset for the study, which has been published in Zenodo ([Dataset for Study (ISL2014BASELINE) MISSING LINK]())

```{r, message=FALSE, warning=FALSE}
require(ggplot2)
require(gplots)
require(reshape2)
require(plyr)
# We load the useful scripts and packages needed throughout the report
source("./lib/rollingWindows.R")
source("./lib/loadIndex.R")
source("./lib/extremeLoadExtraction.R")
source("./lib/aggregateEpisodeData.R")
source("./lib/generateLoadGraphs.R")
source("./lib/generateVideoSnippetScript.R")
source("./lib/codeCount.R")

rootdir <- getwd()
# If not present already, download dataset and uncompress it
setwd(paste(rootdir,"/data",sep=""))
# TODO: complete once the datasets are uploaded
# if(!file.exists())...
# unzip/bz2...
```

We now have all the datafiles uncompressed. This includes mainly a time series of eyetracking data for each of the two sessions, plus an additional set of files with the fixation details and saccade details of each session. There is also another file with the video-codes assigned to a fraction of the of 10-second episodes (those with extreme load indices), as coded by a single human researcher, following the three-dimension video coding scheme mentioned above. The raw gaze video data itself has not been made available due to anonymity reasons. 


## 1. From samples to episodes (eyetracking)

Once we have the raw data, we run the first step of the data analysis, calculating the four eyetracking variables of interest (see previous section), over 10-second rolling windows with 5-second slide (see ```./lib/aggregateEpisodeData.R``` and ```./lib/rollingWindows.R``` files for details). With these eye-tracking variables aggregated into 10-second episodes, then we perform a median cut to obtain an estimation of cognitive overload (which we call Load Index), from 0 to 4, for each of those 10-second episodes (see ```./lib/loadIndex.R``` file for details).

```{r, message=FALSE, warning=FALSE}
# We do the preprocessing, which will generate a Rda file with the 10s
# window data, and will return dataset of episodes and aggregated eyetracking measures

cleandatafile <- "ISL2014BASELINE-AggregatedEyetrackData.Rda"
sessions <- c("ISL2014BASELINE-Session1-eyetracking","ISL2014BASELINE-Session2-eyetracking")
eyetrackdata <- data.frame();
if(!file.exists(paste(rootdir,"/data/",cleandatafile,sep=""))){    
    eyetrackdata <- aggregateEpisodeData(sessions)
}else{
    eyetrackdata <- get(load(paste(rootdir,"/data/",cleandatafile,sep="")))
}

# This dataframe will contain the data with the added Load Index data
loaddata <- data.frame()

# We calculate the load index, considering each session separately for the median cut
for(session in sessions){
    newdata <- data.frame()
    data <- eyetrackdata[eyetrackdata$Session == session,]
    # We add the columns with Load Index data
    newdata <- calculateLoadIndexSession(data, meanlabel="value.Mean", sdlabel="value.SD", fixlabel="value.Fix", saclabel="value.Sac")    
    # We join the new data into a dataset with all session's data
    if(length(loaddata)==0) loaddata <- newdata
    else loaddata <- rbind(loaddata,newdata)
}       

# Now, loaddata will contain the data to be summarized/visualized
# We eliminate the NAs
loaddata <- loaddata[complete.cases(loaddata),]
```

We can plot the time series for the different eyetracking metrics and the load index, for each of the sessions (see the file ```./lib/generateLoadGraphs.R```:

```{r, message=FALSE, warning=FALSE, fig.height=12, fig.width=16}
graphs <- plotLoadGraphs(sessions,loaddata)
multiplot(plotlist=graphs, cols=(length(graphs)/5))
```

We can also see how the different load indices are distributed in each session (with this load index calculated on the basis of the respective session's median for each measure):

```{r, message=FALSE, warning=FALSE}
# We do a basic histogram of the amount episodes regarding their Load Index of each session, 
# using its session median to perform the cut (and normalized by the number of episodes in each session)
loadtabSelf <- table(loaddata$Load, loaddata$Session)
countsDataSelf <- melt(loadtabSelf, varnames=c("Load","Session"))
countsDataSelf$num.Episodes <- numeric(nrow(countsDataSelf))
for(session in sessions){
     countsDataSelf[countsDataSelf$Session == session,"num.Episodes"] <- sum(countsDataSelf[countsDataSelf$Session == session , "value"])
}
# We change the values of the session to something shorter, for aesthetic reasons
countsDataSelf$Session <- mapvalues(countsDataSelf$Session, from = sessions, to = c("Session 1", "Session 2"))
# We draw the plot of count proportions for the load index
ggplot(countsDataSelf,aes(x=Load,y=value/num.Episodes,fill=Session))+
    geom_bar(stat="identity",position="dodge")+
    theme(axis.text=element_text(size=15),
        axis.title=element_text(size=15),
        legend.text=element_text(size=15),
        legend.title=element_text(size=15))+
    xlab("Load indices calculated using \nmedians for respective sessions")+
    ylab("Proportions of the load indices")
```

As we can see, in both sessions the distribution of load indices is approximately gaussian (which is to be expected, given the way such index is calculated with respect to the session median for each eye-tracking metric).


## 2. Subjective episode analysis (stimulated recall)

From the time series of 10-second episodes (and their associated load index) generated in the previous step, 20 were selected from sustained high-/low-load periods. The subjective video feed from the teacher's eyetracker for each of these episodes was extracted from the whole-session gaze replay videos, as exported using BeGaze software (using the files ```ISL2014BASELINE-stimulatedrecall-snippetselection.csv``` and ```./lib/generateVideoSnippetScript.R```):

```{r}
# This assumes the videos are placed in the same location as the dataset
# generateVideoSnippetScript(timesFile=paste(rootdir,"/data/ISL2014BASELINE-stimulatedrecall-snippetselection.csv",sep=""),videoDir=".")
# Then, the generated script (extractSnippets.sh) has to be executed to generate the small video files for each selected episode
```

Then, in a post-hoc interview, the teacher was asked to watch the videos and rate her mental effort (from 1-9) in each of these 20 episodes and provide a rationale for each value (which was audio-recorded and then coded manually by a single researcher (see the ```ISL2014BASELINE-stimulatedrecall-audiocoding.csv``` datafile). We then triangulated the teacher's subjective ratings with the load indices computed from the eye-tracking data for each of those 20 episodes:

```{r}
# We merge the subjective and eyetracking load data, and plot the confidence intervals
subjdata <- read.csv(paste(rootdir,"/data/ISL2014BASELINE-stimulatedrecall-ratings.csv",sep=""))
eyedata <- read.csv(paste(rootdir,"/data/ISL2014BASELINE-stimulatedrecall-snippetselection.csv",sep=""))
data <- merge(subjdata,eyedata,by=c("Session","Snippet"))
plotmeans(Subjective.Value ~ Load, data=data)

# We check the significance of this difference
# TODO: Ask Kshitij about this one
```

Thus, we found that the _teacher's subjective ratings were significantly lower for the low load episodes_ than those for the high load episodes. This supports the idea that the teacher's notions of her own cognitive load coincide somehow with the physiological data from eye-tracking.

We can also look at the qualitative analysis of remarks done by the teacher during this think-aloud protocol (see the ```ISL2014BASELINE-stimulatedrecall-audiocoding.csv``` datafile):

```{r, fig.height=6, fig.width=24, warning=FALSE, message=FALSE}
audiocoding <- read.csv(paste(rootdir,"/data/ISL2014BASELINE-stimulatedrecall-audiocoding.csv",sep=""),sep=";")
codecount <- codeCount(audiocoding)

# We plot the different code occurrences, regarding the load from eyetracking, to see contrasts among them
tabEye <- table(codecount$Code, codecount$Eyetrack.Load)
countsEye <- melt(tabEye, varnames=c("Code","Eyetrack.Load"))
ggplot(countsEye,aes(x=Code,y=value,fill=factor(Eyetrack.Load, levels=c("4","0"))))+
    geom_bar(stat="identity",position="dodge")+
    xlab("Codes/Themes")+
    ylab("# appearances")+
    ggtitle("Code counts for high and low eyetracking load")

# Given the small spread of subjective load values, we create 3 categories: 
# low (1-2), mid-high (3-4)
codecount$Subj.Category <- cut(codecount$Subj.Load,breaks=c(-Inf,2.01,Inf))
tabSubj <- table(codecount$Code, codecount$Subj.Category)
countsSubj <- melt(tabSubj, varnames=c("Code","Subj.Category"))
ggplot(countsSubj,aes(x=Code,y=value,fill=Subj.Category))+
    geom_bar(stat="identity",position="dodge")+
    xlab("Codes/Themes")+
    ylab("# appearances")+
    ggtitle("Code counts for high and low subjective load")+
    scale_fill_manual(values=c('green','red'))
```

Thus, we can see that in the higher-load episodes (both subjective and based on eye-tracking) teacher often mentions attempts to model students' understanding, or the need to take a decision about what would be the best immediate course of action. On the other hand, in lower-load episodes (from subjective rating and eye-tracking index), the individual explanations after a student error (what we could term `repairs'), and the technical details or problems in using the technological tools appear more often. Interestingly, the need to make the lesson progress according to plan (i.e., be within the time/curriculum constraints) appears very often in high-load moments, but also appears in low-load ones (maybe indicating a background concern at all times, even in the lower-load moments).


## 3. Objective episode analysis (video coding)


## 4. Session-level analysis


## Summary of results

TODO 

## Conclusions

TODO