---
title: "Studying Teacher Orchestration Load in Technology-Enhanced Classrooms: A Mixed-method Approach and Case Study"
author: "Luis P. Prieto, Kshitij Sharma, Yun Wen & Pierre Dillenbourg"
output: html_document
---

This is an R Markdown document to reproduce the main data preprocessing, analysis and visualization for the homonymous [ECTEL2015 conference](http://ectel2015.httc.de/) paper. Below, we reproduce the abstract of the paper, a summary of the context and methods of the studies, and then the analysis and visualization of results from the studies.

## Abstract

Teacher orchestration of technology-enhanced learning processes plays a major role in students’ outcomes, especially in face-to-face classrooms. However, very few studies look into the fine-grained details of how such orchestration unfolds, the challenges and cognitive overload that using technologies at a classroom level pose for teachers. This paper proposes a mixed-method approach to the study of orchestration cognitive load, combining physio-behavioural (eye-tracking) and subjective measurements (questionnaires, stimulated recall interviews). We illustrate the approach by applying it to study the orchestration of two technology-enhanced geometry lessons, by a secondary school teacher. The results of our mixed-method analyses highlight the difficulty of classroom-level (as opposed to individual- or group-level) interactions, and especially modelling students’ progress and understanding. Such insights can be useful in the design of new classroom technologies, and to focus researchers’ attention on critical orchestration episodes during their evaluation.

## Context and Method

The study comprised two secondary education geometry sessions that took place in an international school near Lausanne (Switzerland). The two sessions followed the same general structure, and were run by the same teacher (a practitioner with more than 15 years of teaching experience), with two different cohorts of students, of 22 and 23 students respectively (aged 11-12 years old). During the 80-minute sessions, the teacher guided the students in individual and group work about geometrical figures and tessellations, using laptops and specialized geometry software ([Geometer's Sketchpad](http://www.keycurriculum.com/)), interspersed with small periods of explanation/lecturing. To support her orchestration of the lesson, the teacher was using a projector connected to her computer and the school's usual software for classroom management ([NetSupport School](http://www.netsupportschool.com/)).

We employed multiple data gathering techniques, in order to understand the orchestration process at different granularity levels: 

* **session-level subjective ratings of cognitive load** (including both quantitative questionnaires such as [NASA TLX](http://humansystems.arc.nasa.gov/groups/tlx/) and open-ended questions) were performed just after the session
* **The teacher's gaze** was recorded in-situ using a mobile eye-tracker, for fine-grained physiological measures at the milliseconds-level, but also for later analysis by researchers (at the 10-second episode level)
* An (audio-recorded) **stimulated recall interview** was conducted with the teacher, using the subjective video feed of the lesson to rate a selection of episodes from the lesson, using a think-aloud protocol to explain her rationale for the subjective ratings

The data analysis process will generally be as follows:

1. **From samples to episodes (eyetracking)**. Four eye-tracking measurements which have been related with cognitive load (pupil diameter mean, pupil diameter standard deviation, number of long fixations and average saccade speed) are calculated throughout the session. Each of these four measurements is aggregated into 10-second episodes (thus bridging from sample- to episode-level), and the median value of these aggregated measurements for the session is calculated. By performing a median cut (i.e., for each 10-second episode, how many of the four measurements are over the session's median), a ``load index'' ranging from 0 to 4 is calculated, indicating the likelihood that a certain 10-second episode represented a higher load than the session average (see [Prieto et al. (2014)](http://dl.acm.org/citation.cfm?id=2669543) for a more detailed explanation of this process).
2. **Subjective episode analysis (stimulated recall)**. From this collection of 10-second episodes (and their associated load index based on eye-tracking measures), a subset (e.g., 10 episodes) is selected from consistently high- or low-load periods during the session (five each), and 10-second video snippets with the subjective camera view of the teacher are generated for each. These videos are then used in a post-hoc stimulated recall interview with the teacher, in which the teacher is asked to rate subjectively each snippet using a standard 9-level mental effort scale, using a think-aloud protocol to express the rationale behind each rating. The numerical ratings are used to triangulate the load index obtained using eye-tracking, and the think-aloud output goes through qualitative analysis for triangulation and interpretation at higher levels.
3. **Objective episode analysis (video coding)**. All the extreme load index episodes (ELEs, i.e., those 10-second episodes with values 0 or 4) are then video coded by the researcher team, along three main dimensions characterizing orchestration (see table below): the _activity_ being performed by the teacher (e.g., explanation, monitoring), the _social plane_ of the interaction (e.g., class-wide, with individual students) and the _main focus of the teacher's gaze_ (a student laptop, students' faces, etc.). The video code counts are then aggregated for the whole session (thus bridging from the episode- to the session-level), and statistical tests (Pearson's chi-squared) are used to determine which coding dimensions and which video codes contribute most significantly to the differences between high- and low-load episodes, and using these codes to create distinct profiles for each. 
4. **Session-level analysis**. Finally, the session-level subjective ratings provided by the teacher (both quantitative scales and qualitative open responses) are triangulated with the qualitative data coming from the stimulated recall interview, to understand the overall perception of (cognitive) orchestration load at the session-level. These ratings can also be used to make comparisons among different sessions (e.g., which of two sessions was more difficult, and why). These inter-session comparisons can also be triangulated by comparing the medians used for the load index cut in step 1 above (after normalization by the first-episode values to cancel out physiological or ambiental differences among different sessions). 


Orchestration dimension | Teacher activity | Social plane | Main gaze focus
------------------------|------------------|--------------|----------------
Example video codes | Explanation/Lecturing (EXP), Monitoring (MON), Task distribution or transition (TDT), Technical or conceptual repairs (REP)... | Individual (IND), Small group (GRP), Class-wide (CLS) | Students’ faces (FAC) or backs (BAK), Tabletop surface (TAB), Paper worksheet (PAP)...


## Before starting: Data download

First of all, we download the dataset for the study, which has been published in Zenodo ([Dataset for Study (ISL2014BASELINE) MISSING LINK]())

```{r}
require(ggplot2)
require(reshape2)
require(plyr)
# We load the useful scripts and packages needed throughout the report
source("./lib/rollingWindows.R")
source("./lib/loadIndex.R")
source("./lib/extremeLoadExtraction.R")
source("./lib/aggregateEpisodeData.R")

rootdir <- getwd()
# If not present already, download dataset and uncompress it
setwd(paste(rootdir,"/data",sep=""))
# TODO: complete once the datasets are uploaded
# if(!file.exists())...
# unzip/bz2...
```

We now have all the datafiles uncompressed. This includes mainly a time series of eyetracking data for each of the two sessions, plus an additional set of files with the fixation details and saccade details of each session. There is also another file with the video-codes assigned to a fraction of the of 10-second episodes (those with extreme load indices), as coded by a single human researcher, following the three-dimension video coding scheme mentioned above. The raw gaze video data itself has not been made available due to anonymity reasons. 



## 1. From samples to episodes (eyetracking)

Once we have the raw data, we run the first step of the data analysis, calculating the four eyetracking variables of interest (see previous section), over 10-second rolling windows with 5-second slide (see ```./lib/aggregateEpisodeData.R``` and ```./lib/rollingWindows.R``` files for details). With these eye-tracking variables aggregated into 10-second episodes, then we perform a median cut to obtain an estimation of cognitive overload (which we call Load Index), from 0 to 4, for each of those 10-second episodes (see ```./lib/loadIndex.R``` file for details).

```{r, cache=FALSE, message=FALSE, warning=FALSE}
# We do the preprocessing, which will generate a Rda file with the 10s
# window data, and will return dataset of episodes and aggregated eyetracking measures

cleandatafile <- "ISL2014BASELINE-AggregatedEyetrackData.Rda"
sessions <- c("ISL2014BASELINE-Session1-eyetracking","ISL2014BASELINE-Session2-eyetracking")
eyetrackdata <- data.frame();
if(!file.exists(paste(rootdir,"/data/",cleandatafile,sep=""))){    
    eyetrackdata <- aggregateEpisodeData(sessions)
}else{
    eyetrackdata <- get(load(paste(rootdir,"/data/",cleandatafile,sep="")))
}

# This dataframe will contain the data with the added Load Index data
loaddata <- data.frame()

# We calculate the load index, considering each session separately for the median cut
for(session in sessions){
    newdata <- data.frame()
    data <- eyetrackdata[eyetrackdata$Session == session,]
    # We add the columns with Load Index data
    newdata <- calculateLoadIndexSession(data, meanlabel="value.Mean", sdlabel="value.SD", fixlabel="value.Fix", saclabel="value.Sac")    
    # We join the new data into a dataset with all session's data
    if(length(loaddata)==0) loaddata <- newdata
    else loaddata <- rbind(loaddata,newdata)
}       

# Now, loaddata will contain the data to be summarized/visualized
# We eliminate the NAs
loaddata <- loaddata[complete.cases(loaddata),]

# We do a basic histogram of the amount episodes regarding their Load Index of each session, 
# using its session median to perform the cut (and normalized by the number of episodes in each session)
loadtabSelf <- table(loaddata$Load, loaddata$Session)
countsDataSelf <- melt(loadtabSelf, varnames=c("Load","Session"))
countsDataSelf$num.Episodes <- numeric(nrow(countsDataSelf))
for(session in sessions){
     countsDataSelf[countsDataSelf$Session == session,"num.Episodes"] <- sum(countsDataSelf[countsDataSelf$Session == session , "value"])
}
# We change the values of the session to something shorter, for aesthetic reasons
countsDataSelf$Session <- mapvalues(countsDataSelf$Session, from = sessions, to = c("Session 1", "Session 2"))
# We draw the plot of count proportions for the load index
ggplot(countsDataSelf,aes(x=Load,y=value/num.Episodes,fill=Session))+
    geom_bar(stat="identity",position="dodge")+
    theme(axis.text=element_text(size=15),
        axis.title=element_text(size=15),
        legend.text=element_text(size=15),
        legend.title=element_text(size=15))+
    xlab("Load indices calculated using \nmedians for respective sessions")+
    ylab("Proportions of the load indices")
```

As we can see, in both sessions the distribution of load indices is approximately gaussian (which is to be expected,
given the way such index is calculated with respect to the session median for each eye-tracking metric).






### Summary of results

TODO 

## Conclusions

TODO